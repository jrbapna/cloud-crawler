How Cookies currently work:

1) @cookie_store is initialized with option(:cookies) (set on run_404.rb) (see crawljob.rb)
2) It is then merged with data[:cookies] which contains the most recent cookie state of the parent crawl request
3) In crawljob.rb http = CloudCrawler::HTTP.new(@opts, @cookie_store) sends in the current value of @cookie_store 
so that when it pulls pages in http.fetch_page it actually modifies @cookie_store directly and also
makes the HTTP request with the current cookie!
4) Back in crawljob.rb we set data[:cookies] back to the updated current value of @cookie_store.to_s


To remember: 

1) Browsers have a 'Set-Cookie' header (see http.rb) which is a string that should ONLY BE FED into Webrick::Cookie.parse_set_cookies
2) When converting from a regular "stored" cookie string to Webrick::Cookie use the "parse" function instead (see cookie_store.rb and the type_set_cookies boolean in merge!)

Gotchas:

1) Don't use hashes (just don't) when creating data[:cookies] This never worked for me and is honestly probably a pain in the ass. For example don't try data[:cookies] = @cookie_store or data[:cookies] = @cookie_store.to_hash

A couple reasons: @cookie_store inherits from Hash so there is always some weirdness, and the data hash object might convert all of its values into a hash when submitting to redis (so it does somethign weird). The best is to just make data[:cookies] a string and then use Webbrick::Cookie.parse(string) to reconfigure the cookie

2) Expanding on point 1, when you are debugging, ensure @cookie_store.class is always an instance of CloudCrawler::CookieStore and not an instance of Hash



